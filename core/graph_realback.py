"""
ì§„ì§œ LangGraph ì•„í‚¤í…ì²˜ë¡œ ë¦¬íŒ©í† ë§
- Node: ê° ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ë…ë¦½ì  í•¨ìˆ˜ë¡œ
- Tool: ì™¸ë¶€ ê¸°ëŠ¥ì„ ë„êµ¬ë¡œ ë¶„ë¦¬
- State: ëª¨ë“  ë°ì´í„°ë¥¼ ì¤‘ì•™ ê´€ë¦¬
- Edge: ì¡°ê±´ë¶€ íë¦„ ì œì–´
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, List, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage

from core.state import VictimRecoveryState, create_initial_recovery_state
from config.settings import settings

logger = logging.getLogger(__name__)

# ============================================================================
# TOOLS: ì™¸ë¶€ ê¸°ëŠ¥ë“¤ì„ ë„êµ¬ë¡œ ë¶„ë¦¬
# ============================================================================

@tool
async def emergency_detector_tool(user_input: str) -> Dict[str, Any]:
    """ì‘ê¸‰ ìƒí™© ê°ì§€ ë„êµ¬"""
    emergency_keywords = ["ëˆ", "ì†¡ê¸ˆ", "ë³´ëƒˆ", "ì´ì²´", "ê¸‰í•´", "ì‚¬ê¸°", "ë‹¹í–ˆ"]
    time_critical = ["ë°©ê¸ˆ", "ì§€ê¸ˆ", "ë¶„ì „", "ì‹œê°„ì „", "ì˜¤ëŠ˜"]
    
    urgency = 5
    for keyword in emergency_keywords:
        if keyword in user_input.lower():
            urgency += 3
            break
    
    for keyword in time_critical:
        if keyword in user_input.lower():
            urgency += 2
            break
    
    urgency = min(urgency, 10)
    
    if urgency >= 9:
        response = "ğŸš¨ ë§¤ìš° ê¸´ê¸‰! ì¦‰ì‹œ 112ì‹ ê³ í•˜ê³  1811-0041ë²ˆìœ¼ë¡œ ì—°ë½í•˜ì„¸ìš”!"
    elif urgency >= 8:
        response = "ğŸš¨ ê¸´ê¸‰ìƒí™©! ì§€ê¸ˆ 132ë²ˆìœ¼ë¡œ ì „í™”í•˜ì„¸ìš”!"
    else:
        response = ""
    
    return {
        "urgency": urgency,
        "emergency_response": response,
        "is_emergency": urgency >= 8
    }

@tool
async def assessment_question_generator_tool(step: int) -> Dict[str, Any]:
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ ì§ˆë¬¸ ìƒì„± ë„êµ¬"""
    checklist = [
        "ë³¸ì¸ì´ í”¼í•´ìì¸ê°€ìš”? ë„¤ ë˜ëŠ” ì•„ë‹ˆìš”ë¡œ ë‹µí•´ì£¼ì„¸ìš”.",
        "ì§€ê¸ˆë„ ê³„ì† ì—°ë½ì´ ì˜¤ê³  ìˆë‚˜ìš”? ë„¤ ë˜ëŠ” ì•„ë‹ˆìš”ë¡œ ë‹µí•´ì£¼ì„¸ìš”.",
        "ëˆì„ ë³´ë‚´ì…¨ë‚˜ìš”? ë„¤ ë˜ëŠ” ì•„ë‹ˆìš”ë¡œ ë‹µí•´ì£¼ì„¸ìš”.",
        "ê³„ì¢Œì§€ê¸‰ì •ì§€ ì‹ ì²­í•˜ì…¨ë‚˜ìš”? ë„¤ ë˜ëŠ” ì•„ë‹ˆìš”ë¡œ ë‹µí•´ì£¼ì„¸ìš”.",
        "112 ì‹ ê³ í•˜ì…¨ë‚˜ìš”? ë„¤ ë˜ëŠ” ì•„ë‹ˆìš”ë¡œ ë‹µí•´ì£¼ì„¸ìš”.",
        "PASS ì•± ì„¤ì¹˜ë˜ì–´ ìˆë‚˜ìš”? ë„¤ ë˜ëŠ” ì•„ë‹ˆìš”ë¡œ ë‹µí•´ì£¼ì„¸ìš”."
    ]
    
    if step < len(checklist):
        return {
            "question": checklist[step],
            "step": step,
            "is_complete": False
        }
    else:
        return {
            "question": "",
            "step": step,
            "is_complete": True
        }

@tool
async def assessment_answer_processor_tool(answer: str, step: int, responses: Dict) -> Dict[str, Any]:
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ ë‹µë³€ ì²˜ë¦¬ ë„êµ¬"""
    # ì˜ˆ/ì•„ë‹ˆì˜¤ ì²˜ë¦¬
    yes_words = ["ë„¤", "ì˜ˆ", "ì‘", "ë§", "í–ˆ", "ê·¸ë˜", "ìˆ"]
    no_words = ["ì•„ë‹ˆ", "ì•ˆ", "ëª»", "ì—†", "ì•ˆí–ˆ", "ì‹«"]
    
    if any(word in answer.lower() for word in yes_words):
        processed = "yes"
    elif any(word in answer.lower() for word in no_words):
        processed = "no"
    else:
        processed = "unclear"
    
    # ì‘ë‹µ ì €ì¥
    question_ids = ["victim_status", "immediate_danger", "money_sent", 
                   "account_frozen", "police_report", "pass_app"]
    
    if step < len(question_ids):
        responses[question_ids[step]] = {
            "answer": processed,
            "original": answer
        }
    
    # ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš” ì—¬ë¶€
    immediate_action = ""
    if (responses.get("money_sent", {}).get("answer") == "yes" and 
        responses.get("account_frozen", {}).get("answer") == "no"):
        immediate_action = "ğŸš¨ ê¸´ê¸‰! ì§€ê¸ˆ ì¦‰ì‹œ ì€í–‰ì— ì „í™”í•´ì„œ ê³„ì¢Œì§€ê¸‰ì •ì§€ ì‹ ì²­í•˜ì„¸ìš”!"
    
    return {
        "processed_answer": processed,
        "immediate_action": immediate_action,
        "next_step": step + 1,
        "responses": responses
    }

@tool
async def gemini_ai_tool(user_input: str, context: Dict[str, Any]) -> Dict[str, Any]:
    """Gemini AI í˜¸ì¶œ ë„êµ¬"""
    try:
        from services.gemini_assistant import gemini_assistant
        
        if not gemini_assistant.is_enabled:
            return {"response": "", "source": "gemini_disabled"}
        
        result = await asyncio.wait_for(
            gemini_assistant.analyze_and_respond(user_input, context),
            timeout=3.0
        )
        
        return {
            "response": result.get("response", ""),
            "source": "gemini",
            "success": True
        }
        
    except Exception as e:
        logger.warning(f"Gemini ë„êµ¬ ì˜¤ë¥˜: {e}")
        return {"response": "", "source": "gemini_error", "success": False}

@tool
async def rule_based_responder_tool(user_input: str) -> Dict[str, Any]:
    """ë£° ê¸°ë°˜ ì‘ë‹µ ë„êµ¬"""
    user_lower = user_input.lower()
    
    patterns = {
        "prevention": {
            "keywords": ["ì˜ˆë°©", "ë¯¸ë¦¬", "ì„¤ì •", "ë§‰ê¸°"],
            "response": "PASS ì•±ì—ì„œ ëª…ì˜ë„ìš©ë°©ì§€ì„œë¹„ìŠ¤ë¥¼ ì‹ ì²­í•˜ì„¸ìš”."
        },
        "post_damage": {
            "keywords": ["ë‹¹í–ˆ", "í”¼í•´", "ì‚¬ê¸°", "ëˆ", "ì†¡ê¸ˆ"],
            "response": "ì¦‰ì‹œ 132ë²ˆìœ¼ë¡œ ì‹ ê³ í•˜ê³  1811-0041ë²ˆìœ¼ë¡œ ì§€ì› ì‹ ì²­í•˜ì„¸ìš”."
        },
        "suspicious": {
            "keywords": ["ì˜ì‹¬", "ì´ìƒ", "í™•ì¸", "ë§ë‚˜"],
            "response": "ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ ì ˆëŒ€ ì‘ë‹µí•˜ì§€ ë§ˆì‹œê³  132ë²ˆìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”."
        }
    }
    
    for pattern_type, data in patterns.items():
        if any(keyword in user_lower for keyword in data["keywords"]):
            return {
                "response": data["response"],
                "pattern": pattern_type,
                "source": "rule_based"
            }
    
    return {
        "response": "ìƒí™©ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œë©´ ë” ì •í™•í•œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "pattern": "default",
        "source": "rule_based"
    }

@tool
async def hybrid_decision_tool(user_input: str, context: Dict[str, Any]) -> Dict[str, Any]:
    """í•˜ì´ë¸Œë¦¬ë“œ ì˜ì‚¬ê²°ì • ë„êµ¬"""
    try:
        from core.hybrid_decision import SimplifiedHybridDecisionEngine
        
        engine = SimplifiedHybridDecisionEngine(debug=False)
        decision = engine.should_use_gemini(user_input, context)
        
        return {
            "use_gemini": decision.get("use_gemini", False),
            "confidence": decision.get("confidence", 0.0),
            "reasons": decision.get("reasons", [])
        }
    except Exception as e:
        logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ê²°ì • ì˜¤ë¥˜: {e}")
        return {"use_gemini": False, "confidence": 0.0, "reasons": ["error"]}

@tool
async def tts_delivery_tool(text: str) -> Dict[str, Any]:
    """TTS ì „ë‹¬ ë„êµ¬"""
    try:
        from services.tts_service import tts_service
        from services.audio_manager import audio_manager
        
        if not tts_service.is_enabled:
            logger.info(f"ğŸ¤– {text}")
            return {"delivered": True, "method": "console"}
        
        # TTS ìŠ¤íŠ¸ë¦¼ ìƒì„±
        audio_stream = tts_service.text_to_speech_stream(text)
        
        # ì˜¤ë””ì˜¤ ì¬ìƒ
        await audio_manager.play_audio_stream(audio_stream)
        
        return {"delivered": True, "method": "tts"}
        
    except Exception as e:
        logger.error(f"TTS ì „ë‹¬ ì˜¤ë¥˜: {e}")
        # í´ë°±: ì½˜ì†” ì¶œë ¥
        logger.info(f"ğŸ¤– {text}")
        return {"delivered": True, "method": "console_fallback"}

# ============================================================================
# NODES: ê° ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ë…ë¦½ì  í•¨ìˆ˜ë¡œ
# ============================================================================

async def greeting_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """ì¸ì‚¬ ë…¸ë“œ - ì´ˆê¸° ëª¨ë“œ ì„ íƒ ì•ˆë‚´"""
    greeting_message = """ì•ˆë…•í•˜ì„¸ìš”. ë³´ì´ìŠ¤í”¼ì‹± ìƒë‹´ì„¼í„°ì…ë‹ˆë‹¤.
1ë²ˆì€ í”¼í•´ ìƒí™© ì²´í¬ë¦¬ìŠ¤íŠ¸ (ë‹¨ê³„ë³„ í™•ì¸)
2ë²ˆì€ ë§ì¶¤í˜• ìƒë‹´ (ìƒí™©ì— ë§ëŠ” ì¡°ì¹˜)
1ë²ˆ ë˜ëŠ” 2ë²ˆì´ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”."""
    
    # TTS ì „ë‹¬
    await tts_delivery_tool.ainvoke({"text": greeting_message})
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state["messages"].append(AIMessage(content=greeting_message))
    state["current_step"] = "greeting_complete"
    
    return state

async def mode_detection_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """ëª¨ë“œ ê°ì§€ ë…¸ë“œ"""
    last_message = state["messages"][-1] if state["messages"] else None
    user_input = last_message.content if isinstance(last_message, HumanMessage) else ""
    
    # ëª¨ë“œ ê°ì§€
    user_lower = user_input.lower().strip()
    
    if any(pattern in user_lower for pattern in ["1ë²ˆ", "1", "ì²«ë²ˆì§¸", "í”¼í•´", "ì²´í¬"]):
        detected_mode = "assessment"
    elif any(pattern in user_lower for pattern in ["2ë²ˆ", "2", "ë‘ë²ˆì§¸", "ìƒë‹´", "ëŒ€í™”"]):
        detected_mode = "consultation"
    else:
        detected_mode = "unknown"
    
    state["conversation_mode"] = detected_mode
    state["current_step"] = "mode_detected"
    
    return state

async def emergency_detection_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """ì‘ê¸‰ ìƒí™© ê°ì§€ ë…¸ë“œ"""
    last_message = state["messages"][-1] if state["messages"] else None
    user_input = last_message.content if isinstance(last_message, HumanMessage) else ""
    
    # ì‘ê¸‰ ìƒí™© ê°ì§€
    emergency_result = await emergency_detector_tool.ainvoke({"user_input": user_input})
    
    state["urgency_level"] = emergency_result["urgency"]
    
    if emergency_result["is_emergency"]:
        # ì¦‰ì‹œ ì‘ê¸‰ ì‘ë‹µ
        await tts_delivery_tool.ainvoke({"text": emergency_result["emergency_response"]})
        
        state["messages"].append(AIMessage(content=emergency_result["emergency_response"]))
        state["current_step"] = "emergency_handled"
    
    return state

async def assessment_question_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """í‰ê°€ ì§ˆë¬¸ ë…¸ë“œ"""
    current_step = state.get("assessment_step", 0)
    
    # ì§ˆë¬¸ ìƒì„±
    question_result = await assessment_question_generator_tool.ainvoke({"step": current_step})
    
    if not question_result["is_complete"]:
        question = question_result["question"]
        
        # TTS ì „ë‹¬
        await tts_delivery_tool.ainvoke({"text": question})
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["messages"].append(AIMessage(content=question))
        state["current_step"] = "assessment_waiting"
    else:
        # í‰ê°€ ì™„ë£Œ
        state["current_step"] = "assessment_complete"
    
    return state

async def assessment_answer_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """í‰ê°€ ë‹µë³€ ì²˜ë¦¬ ë…¸ë“œ"""
    last_message = state["messages"][-1] if state["messages"] else None
    user_input = last_message.content if isinstance(last_message, HumanMessage) else ""
    
    current_step = state.get("assessment_step", 0)
    responses = state.get("assessment_responses", {})
    
    # ë‹µë³€ ì²˜ë¦¬
    answer_result = await assessment_answer_processor_tool.ainvoke({
        "answer": user_input,
        "step": current_step,
        "responses": responses
    })
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state["assessment_step"] = answer_result["next_step"]
    state["assessment_responses"] = answer_result["responses"]
    
    # ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”ì‹œ
    if answer_result["immediate_action"]:
        await tts_delivery_tool.ainvoke({"text": answer_result["immediate_action"]})
        state["messages"].append(AIMessage(content=answer_result["immediate_action"]))
    
    return state

async def ai_decision_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """AI vs ë£° ê¸°ë°˜ ê²°ì • ë…¸ë“œ"""
    last_message = state["messages"][-1] if state["messages"] else None
    user_input = last_message.content if isinstance(last_message, HumanMessage) else ""
    
    context = {
        "conversation_turns": state.get("conversation_turns", 0),
        "urgency_level": state.get("urgency_level", 5)
    }
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²°ì •
    decision = await hybrid_decision_tool.ainvoke({
        "user_input": user_input,
        "context": context
    })
    
    state["use_ai_response"] = decision["use_gemini"]
    state["decision_confidence"] = decision["confidence"]
    
    return state

async def ai_consultation_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """AI ìƒë‹´ ë…¸ë“œ"""
    last_message = state["messages"][-1] if state["messages"] else None
    user_input = last_message.content if isinstance(last_message, HumanMessage) else ""
    
    context = {
        "conversation_turns": state.get("conversation_turns", 0),
        "urgency_level": state.get("urgency_level", 5)
    }
    
    # Gemini AI í˜¸ì¶œ
    ai_result = await gemini_ai_tool.ainvoke({
        "user_input": user_input,
        "context": context
    })
    
    if ai_result["success"] and ai_result["response"]:
        response = ai_result["response"]
    else:
        # AI ì‹¤íŒ¨ì‹œ ë£° ê¸°ë°˜ìœ¼ë¡œ í´ë°±
        rule_result = await rule_based_responder_tool.ainvoke({"user_input": user_input})
        response = rule_result["response"]
    
    # ì‘ë‹µ ê¸¸ì´ ì œí•œ
    if len(response) > 80:
        response = response[:77] + "..."
    
    # TTS ì „ë‹¬
    await tts_delivery_tool.ainvoke({"text": response})
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state["messages"].append(AIMessage(content=response))
    state["current_step"] = "consultation"
    
    return state

async def rule_consultation_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """ë£° ê¸°ë°˜ ìƒë‹´ ë…¸ë“œ"""
    last_message = state["messages"][-1] if state["messages"] else None
    user_input = last_message.content if isinstance(last_message, HumanMessage) else ""
    
    # ë£° ê¸°ë°˜ ì‘ë‹µ
    rule_result = await rule_based_responder_tool.ainvoke({"user_input": user_input})
    response = rule_result["response"]
    
    # TTS ì „ë‹¬
    await tts_delivery_tool.ainvoke({"text": response})
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state["messages"].append(AIMessage(content=response))
    state["current_step"] = "consultation"
    
    return state

async def conversation_complete_node(state: VictimRecoveryState) -> VictimRecoveryState:
    """ëŒ€í™” ì™„ë£Œ ë…¸ë“œ"""
    farewell = "ìƒë‹´ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ 132ë²ˆìœ¼ë¡œ ì—°ë½í•˜ì„¸ìš”."
    
    # TTS ì „ë‹¬
    await tts_delivery_tool.ainvoke({"text": farewell})
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state["messages"].append(AIMessage(content=farewell))
    state["current_step"] = "conversation_complete"
    
    return state

# ============================================================================
# ROUTING FUNCTIONS: ì¡°ê±´ë¶€ íë¦„ ì œì–´
# ============================================================================

def route_after_greeting(state: VictimRecoveryState) -> Literal["emergency", "mode_detection"]:
    """ì¸ì‚¬ í›„ ë¼ìš°íŒ…"""
    urgency = state.get("urgency_level", 5)
    
    if urgency >= 9:
        return "emergency"
    return "mode_detection"

def route_after_mode_detection(state: VictimRecoveryState) -> Literal["assessment_question", "ai_decision", "greeting"]:
    """ëª¨ë“œ ê°ì§€ í›„ ë¼ìš°íŒ…"""
    mode = state.get("conversation_mode", "unknown")
    
    if mode == "assessment":
        return "assessment_question"
    elif mode == "consultation":
        return "ai_decision"
    else:
        return "greeting"  # ë‹¤ì‹œ ì¸ì‚¬ë¡œ

def route_after_assessment_question(state: VictimRecoveryState) -> Literal["assessment_answer", "ai_decision"]:
    """í‰ê°€ ì§ˆë¬¸ í›„ ë¼ìš°íŒ…"""
    if state.get("current_step") == "assessment_complete":
        return "ai_decision"  # í‰ê°€ ì™„ë£Œ í›„ AIë¡œ ìš”ì•½ ìš”ì²­
    return "assessment_answer"

def route_after_assessment_answer(state: VictimRecoveryState) -> Literal["assessment_question", "ai_decision"]:
    """í‰ê°€ ë‹µë³€ í›„ ë¼ìš°íŒ…"""
    question_result = assessment_question_generator_tool.invoke({"step": state.get("assessment_step", 0)})
    
    if question_result["is_complete"]:
        return "ai_decision"  # í‰ê°€ ì™„ë£Œ -> AI ìš”ì•½
    return "assessment_question"  # ë‹¤ìŒ ì§ˆë¬¸

def route_after_ai_decision(state: VictimRecoveryState) -> Literal["ai_consultation", "rule_consultation"]:
    """AI ê²°ì • í›„ ë¼ìš°íŒ…"""
    if state.get("use_ai_response", False):
        return "ai_consultation"
    return "rule_consultation"

def route_after_consultation(state: VictimRecoveryState) -> Literal["ai_decision", "conversation_complete"]:
    """ìƒë‹´ í›„ ë¼ìš°íŒ…"""
    turns = state.get("conversation_turns", 0)
    
    if turns >= 10:  # ìµœëŒ€ í„´ ì œí•œ
        return "conversation_complete"
    
    return "ai_decision"  # ë‹¤ìŒ ëŒ€í™” í„´

# ============================================================================
# MAIN GRAPH: ì§„ì§œ LangGraph êµ¬ì„±
# ============================================================================

class ProperLangGraphPhishingSystem:
    """ì§„ì§œ LangGraph ì•„í‚¤í…ì²˜ ë³´ì´ìŠ¤í”¼ì‹± ìƒë‹´ ì‹œìŠ¤í…œ"""
    
    def __init__(self, debug: bool = True):
        self.debug = debug
        self.graph = self._build_proper_langgraph()
        
        logger.info("âœ… ì§„ì§œ LangGraph ì•„í‚¤í…ì²˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    
    def _build_proper_langgraph(self) -> StateGraph:
        """ì§„ì§œ LangGraph êµ¬ì„±"""
        
        # StateGraph ìƒì„±
        workflow = StateGraph(VictimRecoveryState)
        
        # ëª¨ë“  ë…¸ë“œ ì¶”ê°€
        workflow.add_node("greeting", greeting_node)
        workflow.add_node("mode_detection", mode_detection_node)
        workflow.add_node("emergency", emergency_detection_node)
        workflow.add_node("assessment_question", assessment_question_node)
        workflow.add_node("assessment_answer", assessment_answer_node)
        workflow.add_node("ai_decision", ai_decision_node)
        workflow.add_node("ai_consultation", ai_consultation_node)
        workflow.add_node("rule_consultation", rule_consultation_node)
        workflow.add_node("conversation_complete", conversation_complete_node)
        
        # ì‹œì‘ì 
        workflow.add_edge(START, "greeting")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ë“¤
        workflow.add_conditional_edges(
            "greeting",
            route_after_greeting,
            {
                "emergency": "emergency",
                "mode_detection": "mode_detection"
            }
        )
        
        workflow.add_conditional_edges(
            "mode_detection", 
            route_after_mode_detection,
            {
                "assessment_question": "assessment_question",
                "ai_decision": "ai_decision",
                "greeting": "greeting"
            }
        )
        
        workflow.add_conditional_edges(
            "assessment_question",
            route_after_assessment_question,
            {
                "assessment_answer": "assessment_answer",
                "ai_decision": "ai_decision"
            }
        )
        
        workflow.add_conditional_edges(
            "assessment_answer",
            route_after_assessment_answer,
            {
                "assessment_question": "assessment_question",
                "ai_decision": "ai_decision"
            }
        )
        
        workflow.add_conditional_edges(
            "ai_decision",
            route_after_ai_decision,
            {
                "ai_consultation": "ai_consultation",
                "rule_consultation": "rule_consultation"
            }
        )
        
        workflow.add_conditional_edges(
            "ai_consultation",
            route_after_consultation,
            {
                "ai_decision": "ai_decision",
                "conversation_complete": "conversation_complete"
            }
        )
        
        workflow.add_conditional_edges(
            "rule_consultation",
            route_after_consultation,
            {
                "ai_decision": "ai_decision", 
                "conversation_complete": "conversation_complete"
            }
        )
        
        # ì‘ê¸‰ ë° ì™„ë£ŒëŠ” ì¢…ë£Œ
        workflow.add_edge("emergency", "conversation_complete")
        workflow.add_edge("conversation_complete", END)
        
        return workflow.compile()
    
    async def process_user_input(self, user_input: str, session_id: str = None) -> str:
        """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ - ì§„ì§œ LangGraph ë°©ì‹"""
        
        try:
            # ì„¸ì…˜ ìƒíƒœ ìƒì„± ë˜ëŠ” ì¡°íšŒ
            if not hasattr(self, '_current_state') or not self._current_state:
                self._current_state = create_initial_recovery_state(
                    session_id or f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                )
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            self._current_state["messages"].append(HumanMessage(content=user_input))
            self._current_state["conversation_turns"] = self._current_state.get("conversation_turns", 0) + 1
            
            if self.debug:
                logger.info(f"ğŸ¯ LangGraph ì²˜ë¦¬: {user_input}")
            
            # LangGraph ì‹¤í–‰ (ì§„ì§œ ê·¸ë˜í”„ ì‚¬ìš©!)
            result_state = await self.graph.ainvoke(self._current_state)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self._current_state = result_state
            
            # ë§ˆì§€ë§‰ AI ì‘ë‹µ ì¶”ì¶œ
            ai_messages = [msg for msg in result_state["messages"] if isinstance(msg, AIMessage)]
            last_response = ai_messages[-1].content if ai_messages else "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            if self.debug:
                logger.info(f"âœ… LangGraph ì‘ë‹µ: {last_response}")
            
            return last_response
            
        except Exception as e:
            logger.error(f"âŒ LangGraph ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return "ì¼ì‹œì  ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 132ë²ˆìœ¼ë¡œ ì—°ë½ì£¼ì„¸ìš”."
    
    def get_current_state(self) -> VictimRecoveryState:
        """í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
        return getattr(self, '_current_state', None)
    
    def reset_conversation(self):
        """ëŒ€í™” ë¦¬ì…‹"""
        self._current_state = None
        logger.info("ğŸ”„ ëŒ€í™” ìƒíƒœ ë¦¬ì…‹")
    
    async def cleanup(self):
        """ì •ë¦¬"""
        if hasattr(self, '_current_state'):
            del self._current_state
        logger.info("ğŸ§¹ LangGraph ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")

# ============================================================================
# ê¸°ì¡´ conversation_manager.pyì™€ì˜ í†µí•©
# ============================================================================

class LangGraphConversationManager:
    """LangGraph ê¸°ë°˜ ëŒ€í™” ë§¤ë‹ˆì € - ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ì™„ë²½ í˜¸í™˜"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        
        # LangGraph ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.ai_brain = ProperLangGraphPhishingSystem(debug=settings.DEBUG)
        
        # ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ë“¤
        from services.tts_service import tts_service
        from services.audio_manager import audio_manager
        self.tts_service = tts_service
        self.audio_manager = audio_manager
        
        # STT ì„¤ì •
        self.stt_client = None
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_running = False
        self.is_processing = False
        self.initialization_complete = False
        
        # ì½œë°±
        self.callbacks = {}
        
        logger.info("âœ… LangGraph ëŒ€í™” ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def initialize(self) -> bool:
        """ì´ˆê¸°í™” (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)"""
        try:
            # 1. ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
            if not self.audio_manager.initialize_output():
                logger.error("âŒ ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            # 2. LangGraph AI ë‘ë‡Œ í…ŒìŠ¤íŠ¸
            test_response = await self.ai_brain.process_user_input("ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
            if test_response:
                logger.info("âœ… LangGraph AI ë‘ë‡Œ ì¤€ë¹„ ì™„ë£Œ")
            
            # 3. STT í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            from services.stream_stt import RTZROpenAPIClient
            self.stt_client = RTZROpenAPIClient(self.client_id, self.client_secret)
            
            # 4. ì´ˆê¸° ëŒ€í™” ì‹œì‘ (LangGraphë¡œ!)
            await self.ai_brain.start_conversation()
            
            self.initialization_complete = True
            logger.info("ğŸ‰ LangGraph íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ LangGraph ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def start_conversation(self):
        """ëŒ€í™” ì‹œì‘ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ + LangGraph)"""
        if not await self.initialize():
            logger.error("âŒ LangGraph ì‹œìŠ¤í…œ ì‹œì‘ ì‹¤íŒ¨")
            return
        
        self.is_running = True
        logger.info("ğŸ™ï¸ LangGraph ìŒì„± ëŒ€í™” ì‹œì‘")
        
        try:
            # STT ì…ë ¥ ì‹œì‘
            self._start_stt_input_safe()
            
            # ë©”ì¸ ë£¨í”„ (LangGraph ê¸°ë°˜)
            await self._langgraph_main_loop()
            
        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ì ì¢…ë£Œ")
        except Exception as e:
            logger.error(f"LangGraph ëŒ€í™” ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        finally:
            await self.cleanup()
    
    async def _langgraph_main_loop(self):
        """LangGraph ê¸°ë°˜ ë©”ì¸ ë£¨í”„"""
        consecutive_errors = 0
        max_consecutive_errors = 5
        
        logger.info("ğŸ”„ LangGraph ë©”ì¸ ë£¨í”„ ì‹œì‘")
        
        while self.is_running:
            try:
                # STT ì…ë ¥ í™•ì¸
                user_input = self._get_stt_input()
                
                if user_input and not self.is_processing:
                    logger.info(f"ğŸ‘¤ ì‚¬ìš©ì ì…ë ¥: {user_input}")
                    await self._langgraph_process_input(user_input)
                
                # ëŒ€í™” ì™„ë£Œ í™•ì¸ (LangGraph ë°©ì‹)
                if self.ai_brain.is_conversation_complete():
                    logger.info("âœ… LangGraph ëŒ€í™” ì™„ë£Œ")
                    break
                
                await asyncio.sleep(0.1)
                consecutive_errors = 0
                        
            except Exception as e:
                consecutive_errors += 1
                logger.error(f"LangGraph ë£¨í”„ ì˜¤ë¥˜ ({consecutive_errors}/{max_consecutive_errors}): {e}")
                
                if consecutive_errors >= max_consecutive_errors:
                    logger.error("âŒ LangGraph ë£¨í”„ ì—°ì† ì˜¤ë¥˜ í•œê³„ ì´ˆê³¼")
                    break
                
                await asyncio.sleep(min(consecutive_errors * 0.5, 3.0))
    
    async def _langgraph_process_input(self, user_input: str):
        """LangGraph ê¸°ë°˜ ì…ë ¥ ì²˜ë¦¬"""
        self.is_processing = True
        
        try:
            # ì‚¬ìš©ì ì…ë ¥ ì½œë°±
            if self.callbacks.get('on_user_speech'):
                self.callbacks['on_user_speech'](user_input)
            
            # LangGraphë¡œ ì²˜ë¦¬! (í•µì‹¬)
            ai_response = await self.ai_brain.process_user_input(user_input)
            
            if ai_response:
                logger.info(f"ğŸ¤– LangGraph ì‘ë‹µ: {ai_response}")
                
                # AI ì‘ë‹µ ì½œë°±
                if self.callbacks.get('on_ai_response'):
                    self.callbacks['on_ai_response'](ai_response)
                
                # TTSëŠ” ì´ë¯¸ LangGraph ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨!
                # (tts_delivery_toolì—ì„œ ì²˜ë¦¬)
            
        except Exception as e:
            logger.error(f"LangGraph ì…ë ¥ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # í´ë°± ì‘ë‹µ
            fallback = "ì¼ì‹œì  ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 132ë²ˆìœ¼ë¡œ ì—°ë½ì£¼ì„¸ìš”."
            await self._safe_tts_delivery(fallback)
            
        finally:
            self.is_processing = False
    
    async def _safe_tts_delivery(self, text: str):
        """ì•ˆì „í•œ TTS ì „ë‹¬ (í´ë°±ìš©)"""
        try:
            if self.tts_service.is_enabled:
                audio_stream = self.tts_service.text_to_speech_stream(text)
                await self.audio_manager.play_audio_stream(audio_stream)
            else:
                print(f"ğŸ¤– {text}")
        except Exception as e:
            logger.error(f"í´ë°± TTS ì˜¤ë¥˜: {e}")
            print(f"ğŸ¤– {text}")
    
    def _start_stt_input_safe(self):
        """STT ì…ë ¥ ì‹œì‘ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        # ê¸°ì¡´ STT ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        import threading
        import queue
        
        self.stt_queue = queue.Queue(maxsize=5)
        
        def stt_worker():
            try:
                if self.stt_client:
                    self.stt_client.reset_stream()
                    
                    def transcript_handler(start_time, transcript, is_final=False):
                        if is_final and transcript.alternatives:
                            text = transcript.alternatives[0].text.strip()
                            if text and len(text) > 1:
                                if not self.stt_queue.full():
                                    self.stt_queue.put_nowait(text)
                    
                    self.stt_client.print_transcript = transcript_handler
                    self.stt_client.transcribe_streaming_grpc()
                    
            except Exception as e:
                logger.error(f"STT ì›Œì»¤ ì˜¤ë¥˜: {e}")
        
        self.stt_thread = threading.Thread(target=stt_worker, daemon=True)
        self.stt_thread.start()
    
    def _get_stt_input(self) -> str:
        """STT ì…ë ¥ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.stt_queue.get_nowait()
        except:
            return None
    
    def set_callbacks(self, **callbacks):
        """ì½œë°± ì„¤ì • (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)"""
        self.callbacks.update(callbacks)
    
    def get_conversation_status(self) -> Dict[str, Any]:
        """ëŒ€í™” ìƒíƒœ ì¡°íšŒ (ê¸°ì¡´ + LangGraph ì •ë³´)"""
        base_status = {
            "state": "running" if self.is_running else "idle",
            "is_running": self.is_running,
            "is_processing": self.is_processing,
            "initialization_complete": self.initialization_complete,
            "system_type": "langgraph",  # ìƒˆë¡œìš´ í”Œë˜ê·¸
        }
        
        # LangGraph ìƒíƒœ ì¶”ê°€
        if hasattr(self.ai_brain, '_current_state') and self.ai_brain._current_state:
            langgraph_summary = self.ai_brain.get_conversation_summary(self.ai_brain._current_state)
            base_status.update(langgraph_summary)
        
        return base_status
    
    def get_audio_status(self) -> dict:
        """ì˜¤ë””ì˜¤ ìƒíƒœ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        try:
            return self.audio_manager.get_performance_stats()
        except Exception as e:
            return {'error': str(e), 'langgraph_system': True}
    
    async def cleanup(self):
        """ì •ë¦¬ (ê¸°ì¡´ + LangGraph)"""
        logger.info("ğŸ§¹ LangGraph ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
        
        try:
            self.is_running = False
            
            # LangGraph ì •ë¦¬
            if hasattr(self.ai_brain, 'cleanup'):
                await self.ai_brain.cleanup()
            
            # ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ ì •ë¦¬
            if hasattr(self, 'stt_thread') and self.stt_thread.is_alive():
                self.stt_thread.join(timeout=3)
            
            if self.audio_manager:
                self.audio_manager.cleanup()
            
            logger.info("âœ… LangGraph ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"LangGraph ì •ë¦¬ ì˜¤ë¥˜: {e}")

# ============================================================================
# ê¸°ì¡´ ì½”ë“œì™€ì˜ ì™„ë²½í•œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
# ============================================================================

# ê¸°ì¡´ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
VoiceFriendlyPhishingGraph = ProperLangGraphPhishingSystem
VoiceFriendlyConversationManager = LangGraphConversationManager

# í•˜ìœ„ í˜¸í™˜ì„±
OptimizedVoicePhishingGraph = ProperLangGraphPhishingSystem
StructuredVoicePhishingGraph = ProperLangGraphPhishingSystem
ConversationManager = LangGraphConversationManager

async def main():
    """LangGraph ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    system = ProperLangGraphPhishingSystem(debug=True)
    
    test_inputs = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "1ë²ˆ",  # í‰ê°€ ëª¨ë“œ
        "ë„¤",   # í”¼í•´ìì„
        "ì•„ë‹ˆìš”", # ì—°ë½ ì•ˆ ì˜´
        "ë„¤",   # ëˆ ë³´ëƒ„
        "ì•„ë‹ˆìš”", # ê³„ì¢Œì •ì§€ ì•ˆ í•¨ -> ì¦‰ì‹œ ì¡°ì¹˜!
        "132ë²ˆì´ ë­ì˜ˆìš”?"  # ë³µì¡í•œ ì§ˆë¬¸ -> AI
    ]
    
    for user_input in test_inputs:
        print(f"\nğŸ‘¤ ì‚¬ìš©ì: {user_input}")
        response = await system.process_user_input(user_input)
        print(f"ğŸ¤– ìƒë‹´ì›: {response}")
        
        # í˜„ì¬ ìƒíƒœ í™•ì¸
        current_state = system.get_current_state()
        if current_state:
            print(f"ğŸ“Š ìƒíƒœ: {current_state.get('current_step')} | ëª¨ë“œ: {current_state.get('conversation_mode')}")
    
    await system.cleanup()

if __name__ == "__main__":
    asyncio.run(main())